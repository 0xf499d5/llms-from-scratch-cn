{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce9295b2-182b-490b-8325-83a67c4a001d",
   "metadata": {},
   "source": [
    "# 章节 4：从零开始实现 GPT 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da97ed-e02f-4d7f-b68e-a0eba3716e02",
   "metadata": {},
   "source": [
    "- 在本章中，我们将设计一个类似 GPT 的大型语言模型（LLM）架构；下一章则将聚焦于该模型的训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4f11e0-4434-4979-9dee-e1207df0eb01",
   "metadata": {},
   "source": [
    "<img src=\"figures/mental-model.webp\" width=450px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fe99ab-0bcf-4778-a6b5-6db81fb826ef",
   "metadata": {},
   "source": [
    "## 4.1 设计LLM的架构"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad72d1ff-d82d-4e33-a88e-3c1a8831797b",
   "metadata": {},
   "source": [
    "- 第1章探讨了如GPT与Llama等模型，这些模型基于transformer架构的decoder部分，并按顺序生成文本。\n",
    "- 因此，这些LLM经常被称为decoder-only LLM。\n",
    "- 与传统的深度学习模型相比，LLM更大，这是因为它们有更多的参数，而不是代码量。\n",
    "- 而在LLM的架构中，有许多元素是重复的。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5213e9-bd1c-437e-aee8-f5e8fb717251",
   "metadata": {},
   "source": [
    "<img src=\"figures/mental-model-2.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d43f5e2-fb51-434a-b9be-abeef6b98d99",
   "metadata": {},
   "source": [
    "- 在前几章中，为了方便展示，我们使用了较小的嵌入（embedding）维度来处理token的输入和输出。\n",
    "- 在本章中，我们将考虑与GPT2-small模型类似的嵌入和模型大小。\n",
    "- 我们将具体实现最小的GPT2-small模型（124M参数）的架构，如Radford等人在[《Language Models are Unsupervised Multitask Learners》](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)中概述的那样（注意，GPT2-small的参数量曾被错误的统计为117M参数，后被更正为124M）。\n",
    "- 第6章将展示如何将预训练权重加载到我们实现的GPT2中，并兼容345、762和1542M参数的模型大小。\n",
    "\n",
    "> 译者注：GPT2的论文《Language Models are Unsupervised Multitask Learners》中错误统计了GPT2系列模型的参数量，这一错误后续在模型仓库中被偷偷修正了。\n",
    "> \n",
    "> 错误的参数量：Small (117M)\tMedium (345M)\tLarge (762M)\tXL (1542M)\n",
    ">\n",
    "> 正确的参数量：Small (124M)\tMedium (355M)\tLarge (774M)\tXL (1558M)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21baa14d-24b8-4820-8191-a2808f7fbabc",
   "metadata": {},
   "source": [
    "- 124M参数GPT-2模型的配置细节包括："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed66875-1f24-445d-add6-006aae3c5707",
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,  # 词表大小\n",
    "    \"ctx_len\": 1024,      # 上下文长度\n",
    "    \"emb_dim\": 768,       # 嵌入维度\n",
    "    \"n_heads\": 12,        # 注意力头（attention heads）的数量\n",
    "    \"n_layers\": 12,       # 模型层数\n",
    "    \"drop_rate\": 0.1,     # Dropout rate\n",
    "    \"qkv_bias\": False     # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12fcd28-d210-4c57-8be6-06cfcd5d73a4",
   "metadata": {},
   "source": [
    "- 我们使用简短的变量名以避免后续代码行的过长\n",
    "- \"vocab_size\" 是一个BPE tokenizer（分词器），词表大小为50257个词，这在第二章介绍过\n",
    "- \"ctx_len\" 表示模型支持输入的最大token数量，这数值由第二章中介绍的位置编码决定\n",
    "- \"emb_dim\" 是对输入token的嵌入维度，这里会将输入的每个token都嵌入成768维的向量\n",
    "- \"n_heads\" 是多头注意力机制中的注意力头数，这在第三章中实现过\n",
    "- \"n_layers\" 是模型中transformer blocks的数量，我们将在接下来的部分中实现它。\n",
    "- \"drop_rate\" 是第三章中讨论的dropout机制的强度；0.1表示在训练期间丢弃10％的隐藏神经元以缓解过拟合\n",
    "- \"qkv_bias\" 决定第三章中的多头注意力机制中的Linear层在计算Query（Q），Key（K）和Value（V）张量时是否应包含偏置向量（bias）；当代LLM通常不会启用这个选项，我们也不会；但在第六章中将OpenAI预训练的GPT-2权重加载到我们的实现的模型时，会再次讨论此选项。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adce779-857b-4418-9501-12a7f3818d88",
   "metadata": {},
   "source": [
    "<img src=\"figures/chapter-steps.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "619c2eed-f8ea-4ff5-92c3-feda0f29b227",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class DummyGPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"ctx_len\"], cfg[\"emb_dim\"])\n",
    "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        # 先用空白实现顶替下 TransformerBlock\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[DummyTransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        # 先用空白实现顶替下 LayerNorm\n",
    "        self.final_norm = DummyLayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class DummyTransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        # 略\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 先啥也别干，原样返回\n",
    "        return x\n",
    "\n",
    "\n",
    "class DummyLayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        # 这里的参数只是为了模拟 LayerNorm 接口。\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 先啥也别干，原样返回\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665e8ab-20ca-4100-b9b9-50d9bdee33be",
   "metadata": {},
   "source": [
    "<img src=\"figures/gpt-in-out.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "794b6b6c-d36f-411e-a7db-8ac566a87fee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import torch\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "batch = []\n",
    "\n",
    "txt1 = \"Every effort moves you\"\n",
    "txt2 = \"Every day holds a\"\n",
    "\n",
    "batch.append(torch.tensor(tokenizer.encode(txt1)))\n",
    "batch.append(torch.tensor(tokenizer.encode(txt2)))\n",
    "batch = torch.stack(batch, dim=0)\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "009238cd-0160-4834-979c-309710986bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-1.2034,  0.3201, -0.7130,  ..., -1.5548, -0.2390, -0.4667],\n",
      "         [-0.1192,  0.4539, -0.4432,  ...,  0.2392,  1.3469,  1.2430],\n",
      "         [ 0.5307,  1.6720, -0.4695,  ...,  1.1966,  0.0111,  0.5835],\n",
      "         [ 0.0139,  1.6754, -0.3388,  ...,  1.1586, -0.0435, -1.0400]],\n",
      "\n",
      "        [[-1.0908,  0.1798, -0.9484,  ..., -1.6047,  0.2439, -0.4530],\n",
      "         [-0.7860,  0.5581, -0.0610,  ...,  0.4835, -0.0077,  1.6621],\n",
      "         [ 0.3567,  1.2698, -0.6398,  ..., -0.0162, -0.1296,  0.3717],\n",
      "         [-0.2407, -0.7349, -0.5102,  ...,  2.0057, -0.3694,  0.1814]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = DummyGPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "logits = model(batch)\n",
    "print(\"Output shape:\", logits.shape)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8332a00-98da-4eb4-b882-922776a89917",
   "metadata": {},
   "source": [
    "## 4.2 对激活进行层归一化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066cfb81-d59b-4d95-afe3-e43cf095f292",
   "metadata": {},
   "source": [
    "- 层归一化（Layer normalization），也叫 LayerNorm ([Ba et al. 2016](https://arxiv.org/abs/1607.06450))，会将神经网络层的激活值规范到均值为0，并将其方差归一化为1。\n",
    "- 这稳定了训练过程，并提高了模型的收敛速度。。\n",
    "- Transformer block中多头注意力模块的输入和输出都会应用LayerNorm，一会会实现它；同时，在最终输出层之前也会应用LayerNorm。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314ac47a-69cc-4597-beeb-65bed3b5910f",
   "metadata": {},
   "source": [
    "<img src=\"figures/layernorm.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab49940-6b35-4397-a80e-df8d092770a7",
   "metadata": {},
   "source": [
    "- 咱们用一个简单的网络，输入一个样本看看LayerNorm是怎么工作的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79e1b463-dc3f-44ac-9cdb-9d5b6f64eb9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2260, 0.3470, 0.0000, 0.2216, 0.0000, 0.0000],\n",
      "        [0.2133, 0.2394, 0.0000, 0.5198, 0.3297, 0.0000]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "# 创建两个训练样例，每个样例有5个维度（特征）\n",
    "batch_example = torch.randn(2, 5) \n",
    "\n",
    "layer = nn.Sequential(nn.Linear(5, 6), nn.ReLU())\n",
    "out = layer(batch_example)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fccc29e-71fc-4c16-898c-6137c6ea5d2e",
   "metadata": {},
   "source": [
    "- 计算上面两个输入的均值和方差："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9888f79e-8e69-44aa-8a19-cd34292adbf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[0.1324],\n",
      "        [0.2170]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[0.0231],\n",
      "        [0.0398]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out.mean(dim=-1, keepdim=True)\n",
    "var = out.var(dim=-1, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052eda3e-b395-48c4-acd4-eb8083bab958",
   "metadata": {},
   "source": [
    "- LayerNorm 会对输入样本分别归一化（下图中的行）; 使用`dim=-1`是在最后一个维度（特征维度）而不是行维度（样本数）上进行计算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570db83a-205c-4f6f-b219-1f6195dde1a7",
   "metadata": {},
   "source": [
    "<img src=\"figures/layernorm2.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8ecbc7-eb14-4fa1-b5d0-7e1ff9694f99",
   "metadata": {},
   "source": [
    "- 减去均值并除以方差的平方根（标准差）会使输入在列（特征）维度上的均值为0，方差为1："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9a1d1bb9-3341-4c9a-bc2a-d2489bf89cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized layer outputs:\n",
      " tensor([[ 0.6159,  1.4126, -0.8719,  0.5872, -0.8719, -0.8719],\n",
      "        [-0.0189,  0.1121, -1.0876,  1.5173,  0.5647, -1.0876]],\n",
      "       grad_fn=<DivBackward0>)\n",
      "Mean:\n",
      " tensor([[9.9341e-09],\n",
      "        [1.9868e-08]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "out_norm = (out - mean) / torch.sqrt(var)\n",
    "print(\"Normalized layer outputs:\\n\", out_norm)\n",
    "\n",
    "mean = out_norm.mean(dim=-1, keepdim=True)\n",
    "var = out_norm.var(dim=-1, keepdim=True)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac62b90c-7156-4979-9a79-ce1fb92969c1",
   "metadata": {},
   "source": [
    "- 每个输入的均值都为0，方差都为1；为了提高可读性，我们可以关闭PyTorch的科学计数法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e06c34b-c68a-4b36-afbe-b30eda4eca39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    0.0000],\n",
      "        [    0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.set_printoptions(sci_mode=False)\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fb958-d4ed-43cc-858d-00052bb6b31a",
   "metadata": {},
   "source": [
    "- 在上面，我们对每个输入的特征进行了归一化\n",
    "- 现在，用相同的思路，我们可以实现一个`LayerNorm`类："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3333a305-aa3d-460a-bcce-b80662d464d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56c3908-7544-4808-b8cb-5d0a55bcca72",
   "metadata": {},
   "source": [
    "**缩放和偏移**\n",
    "- 注意，除了通过减去均值并除以方差执行归一化之外，我们还添加了两个可训练参数，一个是 `scale`，另一个是 `shift`。\n",
    "- 初始的 scale（乘以1）和 shift（加0）值没有任何效果；然而，scale 和 shift 是可训练的参数，如果确定这样做可以改善模型在训练任务上的性能，LLM 在训练过程中会自动调整它们。\n",
    "- 这使得模型能够学习适合其处理数据的适当缩放和偏移。\n",
    "- 注意，在计算方差的平方根之前，我们还添加了一个较小的值（eps）；这是为了避免在方差为0时发生分母为0的问题。\n",
    "\n",
    "**有偏方差**\n",
    "- 在上面的方差计算中，设置 `unbiased=False` 意味着用 $\\frac{\\sum_i (x_i - \\bar{x})^2}{n}$ 来计算方差，其中 n 是样本大小（在这里是特征或列数）；这个公式不包括 Bessel 修正（分母是 n-1），因此得到的方差是有偏估计。\n",
    "- 因为LLM的嵌入维度很高，所以使用 n 或 n-1 （有偏或无偏）的区别不大。\n",
    "- 但 GPT-2 在LayerNorm中使用了有偏方差进行训练，为了在后续章节能加载现有的预训练权重，咱需要`unbiased`这个变量做兼容。\n",
    "\n",
    "- 下面手动实现下 LayerNorm："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23b1000a-e613-4b43-bd90-e54deed8d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "ln = LayerNorm(emb_dim=5)\n",
    "out_ln = ln(batch_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94c12de2-1cab-46e0-a099-e2e470353bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean:\n",
      " tensor([[    -0.0000],\n",
      "        [     0.0000]], grad_fn=<MeanBackward1>)\n",
      "Variance:\n",
      " tensor([[1.0000],\n",
      "        [1.0000]], grad_fn=<VarBackward0>)\n"
     ]
    }
   ],
   "source": [
    "mean = out_ln.mean(dim=-1, keepdim=True)\n",
    "var = out_ln.var(dim=-1, unbiased=False, keepdim=True)\n",
    "\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"Variance:\\n\", var)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e136cfc4-7c89-492e-b120-758c272bca8c",
   "metadata": {},
   "source": [
    "<img src=\"figures/overview-after-ln.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11190e7d-8c29-4115-824a-e03702f9dd54",
   "metadata": {},
   "source": [
    "## 4.3 使用GELU激活函数实现前馈神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0585dfb-f21e-40e5-973f-2f63ad5cb169",
   "metadata": {},
   "source": [
    "- 在这一节中，我们将实现一个网络子模块，该模块将作为LLM中Transformer block的一部分\n",
    "- 我们从激活函数开始\n",
    "- 在深度学习中，由于ReLU（Rectified Linear Unit）激活函数在各种神经网络架构中的简单性和有效性，它们经常被使用\n",
    "- 在LLM中，除了ReLU之外，还使用了其他类型的激活函数；其中两个值得注意的例子是GELU（Gaussian Error Linear Unit）和SwiGLU（Sigmoid-Weighted Linear Unit）\n",
    "- GELU和SwiGLU是更复杂的、平滑的激活函数，它们分别结合了高斯和Sigmoid门控线性单元，为深度学习模型提供了更好的性能，与ReLU的简单分段线性函数不同"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d482ce7-e493-4bfc-a820-3ea99f564ebc",
   "metadata": {},
   "source": [
    "- GELU ([Hendrycks and Gimpel 2016](https://arxiv.org/abs/1606.08415))用多种实现；其精确版本定义为$GELU(x)=x\\cdot \\phi(x)$，其中$\\phi(x)$是标准高斯分布的累积分布函数。\n",
    "- 在实际应用中，常常采用计算成本较低的近似形式：$\\text{GELU}(x) \\approx 0.5 \\cdot x \\cdot \\left(1 + \\tanh\\left[\\sqrt{\\frac{2}{\\pi}} \\cdot \\left(x + 0.044715 \\cdot x^3\\right)\\right]\\right)$（原始的GPT-2模型也是使用这个近似形式进行训练的）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f84694b7-95f3-4323-b6d6-0a73df278e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc5487d2-2576-4118-80a7-56c4caac2e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x300 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "gelu, relu = GELU(), nn.ReLU()\n",
    "\n",
    "# Some sample data\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y_gelu, y_relu = gelu(x), relu(x)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "for i, (y, label) in enumerate(zip([y_gelu, y_relu], [\"GELU\", \"ReLU\"]), 1):\n",
    "    plt.subplot(1, 2, i)\n",
    "    plt.plot(x, y)\n",
    "    plt.title(f\"{label} activation function\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(f\"{label}(x)\")\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd01662-14cb-43fd-bffd-2d702813de2d",
   "metadata": {},
   "source": [
    "- 显然，ReLU是一个分段线性函数，如果输入是正值，它直接原样输出；否则，输出为零。\n",
    "- GELU是一个平滑的非线性函数，近似于ReLU，但输入为负值时，梯度不为0。\n",
    "- 接下来，让我们实现小型神经网络模块 FeedForward，稍后我们将在LLM的Transformer block中使用它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9275c879-b148-4579-a107-86827ca14d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
    "            nn.Dropout(cfg[\"drop_rate\"])\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c4976e2-0261-418e-b042-c5be98c2ccaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 0.1\n"
     ]
    }
   ],
   "source": [
    "print(GPT_CONFIG_124M[\"emb_dim\"], GPT_CONFIG_124M[\"drop_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcaacfa-3cfc-4c9e-b668-b71a2753145a",
   "metadata": {},
   "source": [
    "<img src=\"figures/ffn.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "928e7f7c-d0b1-499f-8d07-4cadb428a6f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 768])\n"
     ]
    }
   ],
   "source": [
    "ffn = FeedForward(GPT_CONFIG_124M)\n",
    "\n",
    "# input shape: [batch_size, num_token, emb_size]\n",
    "x = torch.rand(2, 3, 768) \n",
    "out = ffn(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8756c5-6b04-443b-93d0-e555a316c377",
   "metadata": {},
   "source": [
    "<img src=\"figures/mental-model-3.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcb905-53c7-4886-87d2-4464c5fecf89",
   "metadata": {},
   "source": [
    "## 4.4 添加Shortcut连接"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5161bf8c",
   "metadata": {},
   "source": [
    "- 接下来，我们将探讨shortcut连接，这也被称为跳跃连接或残差连接\n",
    "- 最初，shortcut连接在计算机视觉的深度神经网络(残差网络)中被提出，以缓解消失梯度问题\n",
    "- Shortcut连接为网络中传播的梯度提供了一条更短的路径\n",
    "- 这是通过将一个层的输出加到后面层的输出上来实现，通常跳过中间的一个或多个层\n",
    "- 让我们通过一个小的示例网络来说明这个思想：\n",
    "\n",
    "<img src=\"figures/shortcut-example.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cfd241-a32e-4601-8790-784b82f2f23e",
   "metadata": {},
   "source": [
    "- 示例代码如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "05473938-799c-49fd-86d4-8ed65f94fee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExampleDeepNeuralNetwork(nn.Module):\n",
    "    def __init__(self, layer_sizes, use_shortcut):\n",
    "        super().__init__()\n",
    "        self.use_shortcut = use_shortcut\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Sequential(nn.Linear(layer_sizes[0], layer_sizes[1]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[1], layer_sizes[2]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[2], layer_sizes[3]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[3], layer_sizes[4]), GELU()),\n",
    "            nn.Sequential(nn.Linear(layer_sizes[4], layer_sizes[5]), GELU())\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            # 计算当前层的输出\n",
    "            layer_output = layer(x)\n",
    "            # 检查是否可以使用shortcut\n",
    "            if self.use_shortcut and x.size() == layer_output.size():\n",
    "                x = x + layer_output\n",
    "            else:\n",
    "                x = layer_output\n",
    "        return x\n",
    "\n",
    "\n",
    "def print_gradients(model, x):\n",
    "    # 前向传播\n",
    "    output = model(x)\n",
    "    target = torch.tensor([[0.]])\n",
    "\n",
    "    # 根据输出和标签差距来计算损失\n",
    "    loss = nn.MSELoss()\n",
    "    loss = loss(output, target)\n",
    "    \n",
    "    # 反向传播计算梯度\n",
    "    loss.backward()\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            # 打印权重的平均绝对梯度\n",
    "            print(f\"{name} has gradient mean of {param.grad.abs().mean().item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39bf277-b3db-4bb1-84ce-7a20caff1011",
   "metadata": {},
   "source": [
    "- 让我们先打印**不使用**shortcut连接的梯度值:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c75f43cc-6923-4018-b980-26023086572c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.00020173584925942123\n",
      "layers.1.0.weight has gradient mean of 0.00012011158833047375\n",
      "layers.2.0.weight has gradient mean of 0.0007152041071094573\n",
      "layers.3.0.weight has gradient mean of 0.0013988735154271126\n",
      "layers.4.0.weight has gradient mean of 0.005049645435065031\n"
     ]
    }
   ],
   "source": [
    "layer_sizes = [3, 3, 3, 3, 3, 1]  \n",
    "\n",
    "sample_input = torch.tensor([[1., 0., -1.]])\n",
    "\n",
    "torch.manual_seed(123)\n",
    "model_without_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=False\n",
    ")\n",
    "print_gradients(model_without_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837fd5d4-7345-4663-97f5-38f19dfde621",
   "metadata": {},
   "source": [
    "- 接下来我们打印**使用**shortcut连接的梯度值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11b7c0c2-f9dd-4dd5-b096-a05c48c5f6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.0.weight has gradient mean of 0.22169794142246246\n",
      "layers.1.0.weight has gradient mean of 0.20694106817245483\n",
      "layers.2.0.weight has gradient mean of 0.32896995544433594\n",
      "layers.3.0.weight has gradient mean of 0.2665732502937317\n",
      "layers.4.0.weight has gradient mean of 1.3258541822433472\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model_with_shortcut = ExampleDeepNeuralNetwork(\n",
    "    layer_sizes, use_shortcut=True\n",
    ")\n",
    "print_gradients(model_with_shortcut, sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b385c50b",
   "metadata": {},
   "source": [
    "- 从上述输出可以看出，shortcut连接可以防止梯度在浅层（靠近layer.0）中消失。\n",
    "- 接下来，我们将在实现Transformer块时应用shortcut连接。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8a2072",
   "metadata": {},
   "source": [
    "## 4.5 在transformer块中连接注意力层和线性层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc571b76",
   "metadata": {},
   "source": [
    "- 本节将前述概念融合，搭建transformer块。\n",
    "- Transformer块将前一章的因果多头注意力模块与线性层结合起来，即之前章节中我们实现的前馈神经网络\n",
    "- 此外，transformer块还使用了Dropout和shortcut连接。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e1e8176-e5e3-4152-b1aa-0bbd7891dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from previous_chapters import MultiHeadAttention\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in=cfg[\"emb_dim\"],\n",
    "            d_out=cfg[\"emb_dim\"],\n",
    "            block_size=cfg[\"ctx_len\"],\n",
    "            num_heads=cfg[\"n_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.drop_resid = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 注意力块中的Shortcut连接\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # 与原始输入块求和\n",
    "\n",
    "        # 前馈块中的Shortcut连接\n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = self.drop_resid(x)\n",
    "        x = x + shortcut  # 与原始输入块求和\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b64d16-94a6-4d13-8c85-9494c50478a9",
   "metadata": {},
   "source": [
    "<img src=\"figures/transformer-block.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d3dd26",
   "metadata": {},
   "source": [
    "- 假设我们有2个输入样本，每个样本包含6个token，且每个token都是一个768维的embedding向量。此时，Transformer块会对输入进行自注意力计算，接着进行线性变换，得到一个与输入形状相同的输出。\n",
    "- 我们可以将这个输出视为前一章中所讨论的上下文向量的增强版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fb45a63-b1f3-4b08-b525-dafbc8228405",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT config is: \n",
      "vocab_size: 50257\n",
      "ctx_len: 1024\n",
      "emb_dim: 768\n",
      "n_heads: 12\n",
      "n_layers: 12\n",
      "drop_rate: 0.1\n",
      "qkv_bias: False\n",
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "# print(f\"gpt config is: \\n{GPT_CONFIG_124M}\")\n",
    "print(\"GPT config is: \")\n",
    "for k, v in GPT_CONFIG_124M.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01e737a6-fc99-42bb-9f7e-4da899168811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4, 768])\n",
      "Output shape: torch.Size([2, 4, 768])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "\n",
    "x = torch.rand(2, 4, 768)  # Shape: [batch_size, num_tokens, emb_dim]\n",
    "block = TransformerBlock(GPT_CONFIG_124M)\n",
    "output = block(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Output shape:\", output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f502e4-f3e4-40cb-8268-179eec002394",
   "metadata": {},
   "source": [
    "<img src=\"figures/mental-model-final.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46618527-15ac-4c32-ad85-6cfea83e006e",
   "metadata": {},
   "source": [
    "## 4.6 编写GPT模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a75745",
   "metadata": {},
   "source": [
    "- 我们已经接近成功了：现在让我们将transformer块集成到我们在本章开头编写的架构中，以便获得功能强大的GPT架构\n",
    "- 请注意，transformer块被重复多次使用；在最小的124M GPT-2模型中，我们重复了12次："
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7b362d-f8c5-48d2-8ebd-722480ac5073",
   "metadata": {},
   "source": [
    "<img src=\"figures/gpt.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e4b5d-ed89-4fdf-9a52-67deee0593bc",
   "metadata": {},
   "source": [
    "- 对应的代码实现，其中 `cfg[\"n_layers\"] = 12`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c61de39c-d03c-4a32-8b57-f49ac3834857",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
    "        self.pos_emb = nn.Embedding(cfg[\"ctx_len\"], cfg[\"emb_dim\"])\n",
    "        # self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86571328",
   "metadata": {},
   "source": [
    "- 我们现在可以按照如下方式，采用124M参数模型的配置，以随机初始化权重的方式实例化这个GPT模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "252b78c2-4404-483b-84fe-a412e55c16fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch:\n",
      " tensor([[6109, 3626, 6100,  345],\n",
      "        [6109, 1110, 6622,  257]])\n",
      "\n",
      "Output shape: torch.Size([2, 4, 50257])\n",
      "tensor([[[-0.0055,  0.3224,  0.2185,  ...,  0.2539,  0.4578, -0.4747],\n",
      "         [ 0.2663, -0.2975, -0.5040,  ..., -0.3903,  0.5328, -0.4224],\n",
      "         [ 1.1146, -0.0923,  0.1303,  ...,  0.1521, -0.4494,  0.0276],\n",
      "         [-0.8239,  0.1174, -0.2566,  ...,  1.1197,  0.1036, -0.3993]],\n",
      "\n",
      "        [[-0.1027,  0.1752, -0.1048,  ...,  0.2258,  0.1559, -0.8747],\n",
      "         [ 0.2230,  0.1246,  0.0492,  ...,  0.8573, -0.2933,  0.3036],\n",
      "         [ 0.9409,  1.3068, -0.1610,  ...,  0.8244,  0.1763,  0.0811],\n",
      "         [ 0.4395,  0.2753,  0.1540,  ...,  1.3410, -0.3709,  0.1643]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "model = GPTModel(GPT_CONFIG_124M)\n",
    "\n",
    "out = model(batch)\n",
    "print(\"Input batch:\\n\", batch)\n",
    "print(\"\\nOutput shape:\", out.shape)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af09a24f",
   "metadata": {},
   "source": [
    "- 我们将在下一章对这个模型进行训练。\n",
    "- 这里对模型大小做一个快速说明：我们之前提到它是一个拥有124M参数的模型；可以按照以下方式核对这个数字："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "84fb8be4-9d3b-402b-b3da-86b663aac33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 163,009,536\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total number of parameters: {total_params:,}\")\n",
    "# 163,059,793"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1160952b",
   "metadata": {},
   "source": [
    "- 正如我们看到的，这个模型的参数量为163M个，而不是124M个；为什么呢？\n",
    "- 在原始的GPT-2论文中，研究人员使用了权重绑定，这意味着他们将token嵌入层（tok_emb）重复用作输出层，即设置`self.out_head.weight = self.tok_emb.weight`\n",
    "- token嵌入层将50,257维输入token的one-hot编码投影到768维的embedding表示中\n",
    "- 输出层将768维的embedding投影回到50,257维的表示中，以便我们可以将其转换回单词（更多关于此的信息请参见下一节）\n",
    "- 因此，embedding层和输出层有相同数量的权重参数，正如我们根据其权重矩阵的形状所看到的那样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e3b43233-e9b8-4f5a-b72b-a263ec686982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token embedding layer shape: torch.Size([50257, 768])\n",
      "Output layer shape: torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "print(\"Token embedding layer shape:\", model.tok_emb.weight.shape)\n",
    "print(\"Output layer shape:\", model.out_head.weight.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029a0dc9",
   "metadata": {},
   "source": [
    "- 在原始的GPT-2论文中，研究人员将标记嵌入矩阵重复用作输出矩阵\n",
    "- 因此，如果我们减去输出层的参数数量，就会得到一个124M参数的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "95a22e02-50d3-48b3-a4e0-d9863343c164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable parameters considering weight tying: 124,412,160\n"
     ]
    }
   ],
   "source": [
    "total_params_gpt2 =  total_params - sum(p.numel() for p in model.out_head.parameters())\n",
    "print(f\"Number of trainable parameters considering weight tying: {total_params_gpt2:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1e245d",
   "metadata": {},
   "source": [
    "- 在实践中，我发现在没有权重绑定时训练模型更容易，这就是为什么在这里我们没有实现它的原因。\n",
    "- 然而，在第六章加载预训练权重时，我们将重新审视并应用这个权重绑定的想法。\n",
    "- 最后，我们可以按以下方式计算模型的内存需求，这可以作为一个有用的参考点："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5131a752-fab8-4d70-a600-e29870b33528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 621.83 MB\n"
     ]
    }
   ],
   "source": [
    "# 计算总字节大小（假设每个参数均为占用4个字节的float32类型） \n",
    "total_size_bytes = total_params * 4\n",
    "\n",
    "# 转换为兆字节（MB）\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "\n",
    "print(f\"Total size of the model: {total_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9523897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tok_emb.weight: 38597376\n",
      "pos_emb.weight: 786432\n",
      "final_norm.scale: 768\n",
      "final_norm.shift: 768\n",
      "out_head.weight: 38597376\n",
      "trf_block: 85026816\n"
     ]
    }
   ],
   "source": [
    "# 具体查看GPT模型各个模块的参数量\n",
    "trf_blocks_params_num = 0\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"trf_\"):\n",
    "        trf_blocks_params_num += param.numel()\n",
    "    else:\n",
    "        print(f\"{name}: {param.numel()}\")\n",
    "print(f\"trf_block: {trf_blocks_params_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309a3be4-c20a-4657-b4e0-77c97510b47c",
   "metadata": {},
   "source": [
    "- 练习：你可以尝试实现以下其他配置，这些配置也在 [GPT-2 论文](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)中提到.\n",
    "\n",
    "    - **GPT2-small** (我们已经实现的124M参数配置):\n",
    "        - \"emb_dim\" = 768\n",
    "        - \"n_layers\" = 12\n",
    "        - \"n_heads\" = 12\n",
    "\n",
    "    - **GPT2-medium:**\n",
    "        - \"emb_dim\" = 1024\n",
    "        - \"n_layers\" = 24\n",
    "        - \"n_heads\" = 16\n",
    "    \n",
    "    - **GPT2-large:**\n",
    "        - \"emb_dim\" = 1280\n",
    "        - \"n_layers\" = 36\n",
    "        - \"n_heads\" = 20\n",
    "    \n",
    "    - **GPT2-XL:**\n",
    "        - \"emb_dim\" = 1600\n",
    "        - \"n_layers\" = 48\n",
    "        - \"n_heads\" = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5d9bc0-95ab-45d4-9378-417628d86e35",
   "metadata": {},
   "source": [
    "## 4.7 生成文本"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48da5deb-6ee0-4b9b-8dd2-abed7ed65172",
   "metadata": {},
   "source": [
    "- LLMs（如我们上面实现的GPT模型）一次生成一个单词。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caade12a-fe97-480f-939c-87d24044edff",
   "metadata": {},
   "source": [
    "<img src=\"figures/iterative-gen.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d933457",
   "metadata": {},
   "source": [
    "- 下面的 `generate_text_simple` 函数实现了贪婪解码，这是一种简单快速的文本生成方法\n",
    "- 在贪婪解码中，模型在每一步都选择概率最高的单词（或 token）作为其下一个输出（最高的 logits 输出对应于最高的概率，所以我们甚至不需要显式地计算 softmax 函数）\n",
    "- 在下一章中，我们将实现一个更高级的 `generate_text` 函数\n",
    "- 下图描述了 GPT 模型如何在给定输入上下文的情况下生成下一个单词 token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee0f32c-c18c-445e-b294-a879de2aa187",
   "metadata": {},
   "source": [
    "<img src=\"figures/generate-text.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b428a9-8764-4b36-80cd-7d4e00595ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
    "    # idx是当前上下文中的索引数组，形状为(B, T)\n",
    "    for _ in range(max_new_tokens):\n",
    "\n",
    "        # 如果当前上下文超过了支持的长度，就对当前上下文进行截断\n",
    "        # 例如，如果LLM只支持5个token，而上下文长度为10，\n",
    "        # 那么只有最后5个token会被用作上下文\n",
    "\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        \n",
    "        # 获取预测结果\n",
    "        with torch.no_grad():\n",
    "            logits = model(idx_cond)\n",
    "        \n",
    "        # 只关注最后一个时间步\n",
    "        # (batch, n_token, vocab_size)变为(batch, vocab_size)\n",
    "        logits = logits[:, -1, :]  \n",
    "\n",
    "        # 通过softmax函数获得对应的概率\n",
    "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
    "\n",
    "        # 获取概率值最高的单词索引\n",
    "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
    "\n",
    "        # 将采样到的索引添加到当前运行的上下文索引序列中\n",
    "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
    "\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515f2c1-3cc7-421c-8d58-cc2f563b7030",
   "metadata": {},
   "source": [
    "- 上述的 `generate_text_simple` 函数实现了一次迭代过程，它一次生成一个token。\n",
    "\n",
    "<img src=\"figures/iterative-generate.webp\" width=350px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f682eac4-f9bd-438b-9dec-6b1cc7bc05ce",
   "metadata": {},
   "source": [
    "- 让我们准备一个输入示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3ffc8e-f95f-4a24-a978-939b8953ea3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.4929,  4.4812, -1.6093], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([    0.0000,     0.0012,     0.0000,  ...,     0.0000,     0.0000,\n",
       "            0.0000], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = logits[0, -1, :]\n",
    "b[0] = -1.4929\n",
    "b[1] = 4.4812\n",
    "b[2] = -1.6093\n",
    "\n",
    "print(b[:3])\n",
    "torch.softmax(b, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7e3e94-df0f-4c0f-a6a1-423f500ac1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: [15496, 11, 314, 716]\n",
      "encoded_tensor.shape: torch.Size([1, 4])\n"
     ]
    }
   ],
   "source": [
    "start_context = \"Hello, I am\"\n",
    "\n",
    "encoded = tokenizer.encode(start_context)\n",
    "print(\"encoded:\", encoded)\n",
    "\n",
    "encoded_tensor = torch.tensor(encoded).unsqueeze(0)\n",
    "print(\"encoded_tensor.shape:\", encoded_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72a9b60-de66-44cf-b2f9-1e638934ada4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[15496,    11,   314,   716, 27018, 24086, 47843, 30961, 42348,  7267]])\n",
      "Output length: 10\n"
     ]
    }
   ],
   "source": [
    "model.eval() # 关闭 dropout\n",
    "\n",
    "out = generate_text_simple(\n",
    "    model=model,\n",
    "    idx=encoded_tensor, \n",
    "    max_new_tokens=6, \n",
    "    context_size=GPT_CONFIG_124M[\"ctx_len\"]\n",
    ")\n",
    "\n",
    "print(\"Output:\", out)\n",
    "print(\"Output length:\", len(out[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d131c00-1787-44ba-bec3-7c145497b2c3",
   "metadata": {},
   "source": [
    "- 移除批次维度并转回文本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053d99f6-5710-4446-8d52-117fb34ea9f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am Featureiman Byeswickattribute argue\n"
     ]
    }
   ],
   "source": [
    "decoded_text = tokenizer.decode(out.squeeze(0).tolist())\n",
    "print(decoded_text)  # 由于分词算法使用的是基于subword粒度的BPE算法，所以解码后得到的序列长度有可能少于解码前的tokens序列长度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31806429",
   "metadata": {},
   "source": [
    "- 请注意，该模型尚未训练；因此上述文本是随机生成的\n",
    "- 我们将在下一章训练这个模型"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
