{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            d_in: int,\n",
    "            d_out: int,\n",
    "            max_num_tokens: int,\n",
    "            num_heads: int,\n",
    "            dropout_rate: float,\n",
    "            with_bias: bool = False,\n",
    "            with_mask: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_out = d_out\n",
    "        self.max_num_tokens = max_num_tokens\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.with_bias = with_bias\n",
    "        self.with_mask = with_mask\n",
    "\n",
    "        if d_out % num_heads != 0:\n",
    "            raise ValueError(f\"d_out必须可以被num_heads整除\")\n",
    "        self.d_head = d_out // num_heads\n",
    "\n",
    "        self.wq = None\n",
    "        self.wk = None\n",
    "        self.wv = None\n",
    "        # self.mask = None\n",
    "        self.dropout = None\n",
    "        self.out_proj = None\n",
    "        self._init_parameters()\n",
    "\n",
    "    def _init_parameters(self):\n",
    "        d_in, d_out, with_bias = self.d_in, self.d_out, self.with_bias\n",
    "        self.wq = torch.nn.Linear(in_features=d_in, out_features=d_out, bias=with_bias)\n",
    "        self.wk = torch.nn.Linear(in_features=d_in, out_features=d_out, bias=with_bias)\n",
    "        self.wv = torch.nn.Linear(in_features=d_in, out_features=d_out, bias=with_bias)\n",
    "\n",
    "        block_size = self.max_num_tokens\n",
    "        if self.with_mask:\n",
    "            mask = torch.triu(torch.ones(block_size, block_size), diagonal=1).bool()\n",
    "            self.register_buffer(name=\"mask\", tensor=mask)  # 保存模型时，也会同时保存掩码\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(p=self.dropout_rate)\n",
    "\n",
    "        self.out_proj = torch.nn.Linear(d_out, d_out)\n",
    "\n",
    "    def forward(self, X: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, num_tokens, d_in = X.shape  # 这里需要获取实际输入数据集的上下文长度\n",
    "        assert num_tokens <= self.max_num_tokens, f\"输入序列长度 {num_tokens} 超过最大允许长度 {self.max_num_tokens}\"\n",
    "        assert d_in == self.d_in, \"输入维度不正确\"\n",
    "\n",
    "        Q, K, V = self._compute_qkv(X)\n",
    "        Q, K, V = self._reshape_qkv(Q, K, V, num_tokens)\n",
    "        Q, K, V = self._transpose_qkv(Q, K, V)\n",
    "\n",
    "        attention_scores = self._compute_attention_scores(Q, K)\n",
    "        if self.with_mask:\n",
    "            attention_scores = self._mask_attention_scores(attention_scores, num_tokens)\n",
    "\n",
    "        attention_weights = self._compute_attention_weights(attention_scores)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        contexts = self._compute_contexts(attention_weights, V)\n",
    "        contexts = self._transpose_contexts(contexts)\n",
    "        contexts = self._reshape_contexts(contexts, num_tokens)\n",
    "        \n",
    "        Y = self.out_proj(contexts)\n",
    "        return Y\n",
    "\n",
    "\n",
    "    def _compute_qkv(self, X: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" \n",
    "        X.shape:        (batch_size, num_tokens, d_in)\n",
    "        Q, K, V.shape:  (batch_size, num_tokens, d_out)\n",
    "        \"\"\"\n",
    "        Q, K, V = self.wq(X), self.wk(X), self.wv(X)\n",
    "        return Q, K, V\n",
    "    \n",
    "    def _reshape_qkv(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor, num_tokens: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" \n",
    "        这里的num_tokens与self.num_tokens是不相同的，\n",
    "        这里的num_tokens指的是输入数据的token数量，\n",
    "        而self.num_tokens是初始化时设定的最大token数。\n",
    "        \"\"\"\n",
    "        batch_size = Q.shape[0]\n",
    "        \"\"\"\n",
    "        Q, K, V.shape:  (batch_size, num_tokens, d_out) -> (batch_size, num_tokens, num_heads, d_head)\n",
    "        \"\"\"\n",
    "        Q = Q.reshape(batch_size, num_tokens, self.num_heads, self.d_head)\n",
    "        K = K.reshape(batch_size, num_tokens, self.num_heads, self.d_head)\n",
    "        V = V.reshape(batch_size, num_tokens, self.num_heads, self.d_head)\n",
    "        return Q, K, V\n",
    "\n",
    "    def _transpose_qkv(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\" \n",
    "        Q, K, V.shape:  (batch_size, num_tokens, num_heads, d_head) -> (batch_size, num_heads, num_tokens, d_head)\n",
    "        \"\"\"\n",
    "        Q = Q.transpose(1, 2).contiguous()\n",
    "        K = K.transpose(1, 2).contiguous()\n",
    "        V = V.transpose(1, 2).contiguous()\n",
    "        return Q, K, V\n",
    "    \n",
    "    def _compute_attention_scores(self, Q: torch.Tensor, K: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"  \n",
    "        attention_scores.shape: (batch_size, num_heads, num_tokens, num_tokens)\n",
    "        \"\"\"\n",
    "        attention_scores = Q @ K.transpose(2, 3)\n",
    "        return attention_scores\n",
    "    \n",
    "    def _mask_attention_scores(self, attention_scores: torch.Tensor, num_tokens: int) -> torch.Tensor:\n",
    "        mask = self.mask[:num_tokens, :num_tokens]\n",
    "        mask = self.mask.reshape(1, 1, num_tokens, num_tokens)  # 实际输入数据的token数量未必就是初始化时的token数\n",
    "        attention_scores.masked_fill_(mask, -torch.inf)\n",
    "        return attention_scores\n",
    "\n",
    "    def _compute_attention_weights(self, attention_scores: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"  \n",
    "        attention_weights.shape: (batch_size, num_heads, num_tokens, num_tokens)\n",
    "        \"\"\"\n",
    "        attention_weights = torch.softmax(attention_scores / self.d_head ** 0.5, dim=-1)\n",
    "        return attention_weights\n",
    "    \n",
    "    def _compute_contexts(self, attention_weights: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"  \n",
    "        contexts.shape: (batch_size, num_heads, num_tokens, d_head)\n",
    "        \"\"\"\n",
    "        contexts = attention_weights @ V\n",
    "        return contexts\n",
    "    \n",
    "    def _transpose_contexts(self, contexts: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        contexts.shape: (batch_size, num_heads, num_tokens, d_head) -> (batch_size, num_tokens, num_heads, d_head)\n",
    "        \"\"\"\n",
    "        contexts = contexts.transpose(1, 2).contiguous()\n",
    "        return contexts\n",
    "    \n",
    "    def _reshape_contexts(self, contexts: torch.Tensor, num_tokens: int) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        contexts.shape: (batch_size, num_tokens, num_heads, d_head) -> (batch_size, num_tokens, d_out)\n",
    "        \"\"\"\n",
    "        batch_size = contexts.shape[0]\n",
    "        contexts = contexts.reshape(batch_size, num_tokens, self.d_out)\n",
    "        return contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X' shape is: torch.Size([2, 6, 3])\n",
      "and C is: \n",
      "tensor([[[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]],\n",
      "\n",
      "        [[0.4300, 0.1500, 0.8900],\n",
      "         [0.5500, 0.8700, 0.6600],\n",
      "         [0.5700, 0.8500, 0.6400],\n",
      "         [0.2200, 0.5800, 0.3300],\n",
      "         [0.7700, 0.2500, 0.1000],\n",
      "         [0.0500, 0.8000, 0.5500]]])\n"
     ]
    }
   ],
   "source": [
    "row = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "X = torch.stack((row, row), dim=0)\n",
    "print(f\"X' shape is: {X.shape}\")\n",
    "print(f\"and C is: \\n{X}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y's shape is: torch.Size([2, 6, 2])\n",
      "Y is: \n",
      "tensor([[[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]],\n",
      "\n",
      "        [[0.3190, 0.4858],\n",
      "         [0.2943, 0.3897],\n",
      "         [0.2856, 0.3593],\n",
      "         [0.2693, 0.3873],\n",
      "         [0.2639, 0.3928],\n",
      "         [0.2575, 0.4028]]], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(123)\n",
    "mha = MultiHeadAttention(\n",
    "    d_in=3,\n",
    "    d_out=2,\n",
    "    max_num_tokens=6,\n",
    "    num_heads=2,\n",
    "    with_bias=False,\n",
    "    with_mask=True,\n",
    "    dropout_rate=0.0\n",
    ")\n",
    "Y = mha(X)\n",
    "print(f\"Y's shape is: {Y.shape}\")\n",
    "print(f\"Y is: \\n{Y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected = torch.tensor(\n",
    "  [[[0.3190, 0.4858],\n",
    "    [0.2943, 0.3897],\n",
    "    [0.2856, 0.3593],\n",
    "    [0.2693, 0.3873],\n",
    "    [0.2639, 0.3928],\n",
    "    [0.2575, 0.4028]],\n",
    "\n",
    "   [[0.3190, 0.4858],\n",
    "    [0.2943, 0.3897],\n",
    "    [0.2856, 0.3593],\n",
    "    [0.2693, 0.3873],\n",
    "    [0.2639, 0.3928],\n",
    "    [0.2575, 0.4028]]]\n",
    ")\n",
    "assert torch.allclose(Y, expected, atol=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
